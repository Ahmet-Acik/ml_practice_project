{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a52de9b",
   "metadata": {},
   "source": [
    "# ğŸ¤– Machine Learning Model Training\n",
    "\n",
    "Welcome to the exciting world of machine learning! In this notebook, we'll:\n",
    "\n",
    "1. **Load** our cleaned and preprocessed data\n",
    "2. **Split** data into training and testing sets\n",
    "3. **Train multiple ML models** (Linear Regression, Random Forest, XGBoost)\n",
    "4. **Compare model performance** with various metrics\n",
    "5. **Select the best model** for our problem\n",
    "6. **Save the trained model** for deployment\n",
    "\n",
    "## ğŸ¯ Learning Goals\n",
    "- Understand train/validation/test splits\n",
    "- Learn different ML algorithms and when to use them\n",
    "- Practice model evaluation and comparison\n",
    "- Understand overfitting and cross-validation\n",
    "- Learn to save and load trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa5f78",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# System libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“¦ Available ML algorithms: Linear Regression, Ridge, Lasso, Random Forest, XGBoost, SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our cleaned and preprocessed data\n",
    "print(\"ğŸ“‚ Loading cleaned datasets...\")\n",
    "\n",
    "# Load scaled data for training\n",
    "df_scaled = pd.read_csv('../data/processed/student_performance_cleaned.csv')\n",
    "print(f\"âœ… Scaled dataset loaded: {df_scaled.shape}\")\n",
    "\n",
    "# Load unscaled data for interpretability\n",
    "df_unscaled = pd.read_csv('../data/processed/student_performance_cleaned_unscaled.csv')\n",
    "print(f\"âœ… Unscaled dataset loaded: {df_unscaled.shape}\")\n",
    "\n",
    "# Load the saved scaler\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "print(f\"âœ… Scaler loaded for future predictions\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Dataset Overview:\")\n",
    "display(df_scaled.head())\n",
    "\n",
    "print(f\"\\nğŸ“Š Features available: {df_scaled.shape[1] - 2} (excluding student_id and exam_score)\")\n",
    "feature_columns = [col for col in df_scaled.columns if col not in ['student_id', 'exam_score']]\n",
    "print(f\"Feature list: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9ef7a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Split Data for Training & Testing\n",
    "\n",
    "We'll split our data into training and testing sets to evaluate model performance properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aab24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variables\n",
    "print(\"ğŸ¯ Preparing Features and Target:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Features (X) and target (y) from scaled data\n",
    "X = df_scaled[feature_columns]\n",
    "y = df_scaled['exam_score']\n",
    "\n",
    "print(f\"ğŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target vector shape: {y.shape}\")\n",
    "print(f\"ğŸ“ˆ Target statistics:\")\n",
    "print(f\"   Mean: {y.mean():.2f}\")\n",
    "print(f\"   Std:  {y.std():.2f}\")\n",
    "print(f\"   Min:  {y.min():.2f}\")\n",
    "print(f\"   Max:  {y.max():.2f}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nâ“ Missing values check:\")\n",
    "print(f\"   Features: {X.isnull().sum().sum()}\")\n",
    "print(f\"   Target: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b060c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print(\"âœ‚ï¸ Splitting Data:\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "# 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=None  # We'll use simple random split for regression\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Testing set:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Target distribution:\")\n",
    "print(f\"   Training - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"   Testing  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(y, bins=20, alpha=0.7, color='blue', label='Full Dataset')\n",
    "plt.title('Full Dataset Distribution')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(y_train, bins=20, alpha=0.7, color='green', label='Training Set')\n",
    "plt.title('Training Set Distribution')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(y_test, bins=20, alpha=0.7, color='red', label='Testing Set')\n",
    "plt.title('Testing Set Distribution')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Data split completed! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947a224",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 3: Train Multiple ML Models\n",
    "\n",
    "Let's train several different algorithms and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize different ML models\n",
    "print(\"ğŸ¤– Initializing ML Models:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'Support Vector Regression': SVR(kernel='rbf', C=1.0)\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“¦ Initialized {len(models)} different models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   â€¢ {name}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Each model will be trained and evaluated using:\")\n",
    "print(\"   â€¢ Mean Squared Error (MSE)\")\n",
    "print(\"   â€¢ Mean Absolute Error (MAE)\")\n",
    "print(\"   â€¢ RÂ² Score (coefficient of determination)\")\n",
    "print(\"   â€¢ Cross-validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and collect results\n",
    "print(\"ğŸš€ Training Models:\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ”„ Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions_train': y_pred_train,\n",
    "        'predictions_test': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {name} trained successfully!\")\n",
    "    print(f\"      Test RÂ²: {test_r2:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ‰ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fe227",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: Model Evaluation & Comparison\n",
    "\n",
    "Let's compare the performance of all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea83b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results comparison table\n",
    "print(\"ğŸ“Š Model Performance Comparison:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "comparison_data = []\n",
    "for name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Train RÂ²': f\"{metrics['train_r2']:.4f}\",\n",
    "        'Test RÂ²': f\"{metrics['test_r2']:.4f}\",\n",
    "        'Test MAE': f\"{metrics['test_mae']:.4f}\",\n",
    "        'Test MSE': f\"{metrics['test_mse']:.4f}\",\n",
    "        'CV Mean RÂ²': f\"{metrics['cv_mean']:.4f}\",\n",
    "        'CV Std': f\"{metrics['cv_std']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test RÂ²', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'])\n",
    "best_r2 = results[best_model_name]['test_r2']\n",
    "\n",
    "print(f\"\\nğŸ† Best performing model: {best_model_name}\")\n",
    "print(f\"ğŸ¯ Best Test RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"ğŸ“ˆ This means the model explains {best_r2*100:.1f}% of the variance in exam scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4409b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "model_names = list(results.keys())\n",
    "\n",
    "# RÂ² Score comparison\n",
    "train_r2_scores = [results[name]['train_r2'] for name in model_names]\n",
    "test_r2_scores = [results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, train_r2_scores, width, label='Train RÂ²', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_r2_scores, width, label='Test RÂ²', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('RÂ² Score')\n",
    "axes[0, 0].set_title('RÂ² Score Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "test_mae_scores = [results[name]['test_mae'] for name in model_names]\n",
    "axes[0, 1].bar(model_names, test_mae_scores, alpha=0.8, color='orange')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[0, 1].set_title('Test MAE Comparison (Lower is Better)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "axes[1, 0].bar(model_names, cv_means, yerr=cv_stds, alpha=0.8, color='green', capsize=5)\n",
    "axes[1, 0].set_xlabel('Models')\n",
    "axes[1, 0].set_ylabel('Cross-Validation RÂ² Score')\n",
    "axes[1, 0].set_title('Cross-Validation Performance')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis (difference between train and test RÂ²)\n",
    "overfitting = [results[name]['train_r2'] - results[name]['test_r2'] for name in model_names]\n",
    "colors = ['red' if x > 0.1 else 'orange' if x > 0.05 else 'green' for x in overfitting]\n",
    "axes[1, 1].bar(model_names, overfitting, alpha=0.8, color=colors)\n",
    "axes[1, 1].set_xlabel('Models')\n",
    "axes[1, 1].set_ylabel('Train RÂ² - Test RÂ²')\n",
    "axes[1, 1].set_title('Overfitting Analysis (Lower is Better)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='Overfitting Threshold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Performance Analysis:\")\n",
    "print(f\"   ğŸ† Highest Test RÂ²: {best_model_name} ({best_r2:.4f})\")\n",
    "print(f\"   ğŸ“‰ Lowest Test MAE: {min(model_names, key=lambda x: results[x]['test_mae'])} ({min(results[x]['test_mae'] for x in model_names):.4f})\")\n",
    "print(f\"   ğŸ¯ Best CV Score: {max(model_names, key=lambda x: results[x]['cv_mean'])} ({max(results[x]['cv_mean'] for x in model_names):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e62df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions for the best model\n",
    "print(f\"ğŸ” Detailed Analysis of Best Model: {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions_test']\n",
    "\n",
    "# Prediction vs Actual scatter plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, best_predictions, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Exam Scores')\n",
    "plt.ylabel('Predicted Exam Scores')\n",
    "plt.title(f'{best_model_name}\\nPredictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - best_predictions\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(best_predictions, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Exam Scores')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.axvline(residuals.mean(), color='red', linestyle='--', label=f'Mean: {residuals.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction statistics\n",
    "print(f\"\\nğŸ“Š Prediction Analysis:\")\n",
    "print(f\"   Mean Residual: {residuals.mean():.4f} (should be close to 0)\")\n",
    "print(f\"   Residual Std: {residuals.std():.4f}\")\n",
    "print(f\"   Max Absolute Error: {abs(residuals).max():.4f}\")\n",
    "print(f\"   95% of predictions within Â±{np.percentile(abs(residuals), 95):.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1e86c",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 5: Feature Importance Analysis\n",
    "\n",
    "Let's understand which features are most important for predicting exam scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "print(\"ğŸ¯ Feature Importance Analysis:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get feature importance from tree-based models\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "available_tree_models = [name for name in tree_models if name in trained_models]\n",
    "\n",
    "if available_tree_models:\n",
    "    plt.figure(figsize=(15, 5 * len(available_tree_models)))\n",
    "    \n",
    "    for i, model_name in enumerate(available_tree_models, 1):\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Get feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            plt.subplot(len(available_tree_models), 1, i)\n",
    "            bars = plt.bar(importance_df['feature'], importance_df['importance'], alpha=0.8)\n",
    "            plt.title(f'Feature Importance - {model_name}')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Importance')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Color the bars by importance\n",
    "            max_importance = importance_df['importance'].max()\n",
    "            for bar, imp in zip(bars, importance_df['importance']):\n",
    "                if imp > 0.7 * max_importance:\n",
    "                    bar.set_color('red')\n",
    "                elif imp > 0.4 * max_importance:\n",
    "                    bar.set_color('orange')\n",
    "                else:\n",
    "                    bar.set_color('lightblue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features for each model\n",
    "    for model_name in available_tree_models:\n",
    "        model = trained_models[model_name]\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nğŸ† Top 5 features for {model_name}:\")\n",
    "            for idx, row in importance_df.head().iterrows():\n",
    "                print(f\"   {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "else:\n",
    "    print(\"No tree-based models available for feature importance analysis.\")\n",
    "\n",
    "# Linear model coefficients\n",
    "if 'Linear Regression' in trained_models:\n",
    "    linear_model = trained_models['Linear Regression']\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': linear_model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Linear Regression Coefficients (Top 5):\")\n",
    "    for idx, row in coefficients.head().iterrows():\n",
    "        direction = \"â†—ï¸\" if row['coefficient'] > 0 else \"â†˜ï¸\"\n",
    "        print(f\"   {row['feature']:20s}: {row['coefficient']:8.4f} {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928d7b6",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 6: Hyperparameter Tuning (Optional)\n",
    "\n",
    "Let's optimize our best model with hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031cefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "print(f\"âš™ï¸ Hyperparameter Tuning for {best_model_name}:\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"ğŸ” Performing Grid Search for {best_model_name}...\")\n",
    "    \n",
    "    # Get the base model\n",
    "    base_model = models[best_model_name]\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model, \n",
    "        param_grid, \n",
    "        cv=5, \n",
    "        scoring='r2', \n",
    "        n_jobs=-1, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_tuned_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    y_pred_tuned = best_tuned_model.predict(X_test)\n",
    "    tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "    tuned_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Tuning Results:\")\n",
    "    print(f\"   Original RÂ² Score: {results[best_model_name]['test_r2']:.4f}\")\n",
    "    print(f\"   Tuned RÂ² Score:    {tuned_r2:.4f}\")\n",
    "    print(f\"   Improvement:       {tuned_r2 - results[best_model_name]['test_r2']:.4f}\")\n",
    "    print(f\"\\n   Original MAE:      {results[best_model_name]['test_mae']:.4f}\")\n",
    "    print(f\"   Tuned MAE:         {tuned_mae:.4f}\")\n",
    "    print(f\"   Improvement:       {results[best_model_name]['test_mae'] - tuned_mae:.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† Best Parameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Update best model if tuned version is better\n",
    "    if tuned_r2 > results[best_model_name]['test_r2']:\n",
    "        print(f\"\\nâœ… Tuned model is better! Using tuned version as final model.\")\n",
    "        final_model = best_tuned_model\n",
    "        final_model_name = f\"{best_model_name} (Tuned)\"\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š Original model performs better. Using original version.\")\n",
    "        final_model = trained_models[best_model_name]\n",
    "        final_model_name = best_model_name\n",
    "        \n",
    "else:\n",
    "    print(f\"âš ï¸ Hyperparameter tuning not available for {best_model_name}\")\n",
    "    final_model = trained_models[best_model_name]\n",
    "    final_model_name = best_model_name\n",
    "\n",
    "print(f\"\\nğŸ‰ Final Model Selected: {final_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9c697",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 7: Save the Best Model\n",
    "\n",
    "Let's save our trained model for future use and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and related information\n",
    "print(\"ğŸ’¾ Saving Final Model:\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_filename = '../models/best_student_performance_model.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"âœ… Model saved: {model_filename}\")\n",
    "\n",
    "# Save model metadata\n",
    "if 'final_model' in locals():\n",
    "    final_predictions = final_model.predict(X_test)\n",
    "    final_r2 = r2_score(y_test, final_predictions)\n",
    "    final_mae = mean_absolute_error(y_test, final_predictions)\n",
    "else:\n",
    "    final_predictions = results[best_model_name]['predictions_test']\n",
    "    final_r2 = results[best_model_name]['test_r2']\n",
    "    final_mae = results[best_model_name]['test_mae']\n",
    "\n",
    "model_metadata = {\n",
    "    'model_name': final_model_name,\n",
    "    'model_type': type(final_model).__name__,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'features': feature_columns,\n",
    "    'performance': {\n",
    "        'test_r2_score': final_r2,\n",
    "        'test_mae': final_mae,\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "    },\n",
    "    'training_data_shape': X_train.shape,\n",
    "    'test_data_shape': X_test.shape\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_filename = '../models/model_metadata.pkl'\n",
    "joblib.dump(model_metadata, metadata_filename)\n",
    "print(f\"âœ… Metadata saved: {metadata_filename}\")\n",
    "\n",
    "# Save feature names for deployment\n",
    "feature_names_filename = '../models/feature_names.pkl'\n",
    "joblib.dump(feature_columns, feature_names_filename)\n",
    "print(f\"âœ… Feature names saved: {feature_names_filename}\")\n",
    "\n",
    "# Create a simple prediction function\n",
    "def predict_exam_score(model, scaler, **features):\n",
    "    \"\"\"\n",
    "    Predict exam score based on student features.\n",
    "    \n",
    "    Parameters:\n",
    "    model: trained ML model\n",
    "    scaler: fitted StandardScaler\n",
    "    **features: student features as keyword arguments\n",
    "    \n",
    "    Returns:\n",
    "    predicted exam score\n",
    "    \"\"\"\n",
    "    # Create DataFrame with features\n",
    "    feature_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for col in feature_columns:\n",
    "        if col not in feature_df.columns:\n",
    "            feature_df[col] = 0  # Default value\n",
    "    \n",
    "    # Reorder columns to match training data\n",
    "    feature_df = feature_df[feature_columns]\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(feature_df)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(scaled_features)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Save prediction function\n",
    "prediction_func_filename = '../models/prediction_function.pkl'\n",
    "joblib.dump(predict_exam_score, prediction_func_filename)\n",
    "print(f\"âœ… Prediction function saved: {prediction_func_filename}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Model Summary:\")\n",
    "print(f\"   Model: {final_model_name}\")\n",
    "print(f\"   Test RÂ² Score: {final_r2:.4f}\")\n",
    "print(f\"   Test MAE: {final_mae:.4f}\")\n",
    "print(f\"   Features: {len(feature_columns)}\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80bac3",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 8: Test the Saved Model\n",
    "\n",
    "Let's test our saved model to make sure it works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31407c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "print(\"ğŸ§ª Testing Saved Model:\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load('../models/best_student_performance_model.pkl')\n",
    "loaded_scaler = joblib.load('../models/scaler.pkl')\n",
    "loaded_features = joblib.load('../models/feature_names.pkl')\n",
    "loaded_metadata = joblib.load('../models/model_metadata.pkl')\n",
    "loaded_predict_func = joblib.load('../models/prediction_function.pkl')\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"ğŸ“Š Model type: {loaded_metadata['model_type']}\")\n",
    "print(f\"ğŸ•’ Training date: {loaded_metadata['training_date']}\")\n",
    "print(f\"ğŸ¯ Test RÂ² Score: {loaded_metadata['performance']['test_r2_score']:.4f}\")\n",
    "\n",
    "# Test prediction with sample data\n",
    "print(f\"\\nğŸ”® Testing Predictions:\")\n",
    "\n",
    "# Create sample students for testing\n",
    "sample_students = [\n",
    "    {\n",
    "        'name': 'High Achiever',\n",
    "        'study_hours': 15.0,\n",
    "        'attendance': 95.0,\n",
    "        'previous_grade': 85.0,\n",
    "        'sleep_hours': 8.0,\n",
    "        'extra_activities': 2,\n",
    "        'family_support': 5,\n",
    "        'study_efficiency': 15.79,\n",
    "        'sleep_quality': 0.93,\n",
    "        'work_life_balance': 0.33,\n",
    "        'preparation_score': 0.85,\n",
    "        'improvement_potential': 15.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Average Student',\n",
    "        'study_hours': 8.0,\n",
    "        'attendance': 75.0,\n",
    "        'previous_grade': 65.0,\n",
    "        'sleep_hours': 7.0,\n",
    "        'extra_activities': 3,\n",
    "        'family_support': 3,\n",
    "        'study_efficiency': 10.67,\n",
    "        'sleep_quality': 1.0,\n",
    "        'work_life_balance': 0.25,\n",
    "        'preparation_score': 0.65,\n",
    "        'improvement_potential': 35.0\n",
    "    },\n",
    "    {\n",
    "        'name': 'Struggling Student',\n",
    "        'study_hours': 3.0,\n",
    "        'attendance': 60.0,\n",
    "        'previous_grade': 45.0,\n",
    "        'sleep_hours': 5.0,\n",
    "        'extra_activities': 5,\n",
    "        'family_support': 2,\n",
    "        'study_efficiency': 5.0,\n",
    "        'sleep_quality': 0.67,\n",
    "        'work_life_balance': 0.17,\n",
    "        'preparation_score': 0.35,\n",
    "        'improvement_potential': 55.0\n",
    "    }\n",
    "]\n",
    "\n",
    "for student in sample_students:\n",
    "    name = student.pop('name')\n",
    "    predicted_score = loaded_predict_func(loaded_model, loaded_scaler, **student)\n",
    "    print(f\"\\nğŸ‘¤ {name}:\")\n",
    "    print(f\"   Study Hours: {student['study_hours']}, Attendance: {student['attendance']:.0f}%\")\n",
    "    print(f\"   Previous Grade: {student['previous_grade']:.0f}, Sleep: {student['sleep_hours']} hours\")\n",
    "    print(f\"   ğŸ¯ Predicted Exam Score: {predicted_score:.1f}\")\n",
    "\n",
    "# Verify model performance on test set\n",
    "loaded_test_predictions = loaded_model.predict(loaded_scaler.transform(X_test))\n",
    "loaded_test_r2 = r2_score(y_test, loaded_test_predictions)\n",
    "\n",
    "print(f\"\\nâœ… Model Verification:\")\n",
    "print(f\"   Original Test RÂ²: {final_r2:.4f}\")\n",
    "print(f\"   Loaded Test RÂ²:   {loaded_test_r2:.4f}\")\n",
    "print(f\"   Match: {'âœ… Yes' if abs(final_r2 - loaded_test_r2) < 0.0001 else 'âŒ No'}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Model successfully saved and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3bfb4f",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 9: Training Summary\n",
    "\n",
    "Let's summarize everything we accomplished in this model training phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb614412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ MACHINE LEARNING TRAINING SUMMARY\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"âœ… COMPLETED TASKS:\")\n",
    "print(f\"   1. ğŸ“‚ Data Loading:\")\n",
    "print(f\"      â€¢ Loaded {df_scaled.shape[0]} cleaned student records\")\n",
    "print(f\"      â€¢ Used {len(feature_columns)} engineered features\")\n",
    "\n",
    "print(f\"\\n   2. âœ‚ï¸ Data Splitting:\")\n",
    "print(f\"      â€¢ Training set: {X_train.shape[0]} samples (80%)\")\n",
    "print(f\"      â€¢ Testing set: {X_test.shape[0]} samples (20%)\")\n",
    "\n",
    "print(f\"\\n   3. ğŸ¤– Model Training:\")\n",
    "print(f\"      â€¢ Trained {len(models)} different algorithms\")\n",
    "print(f\"      â€¢ Linear, Ridge, Lasso, Random Forest, XGBoost, SVM\")\n",
    "print(f\"      â€¢ Used cross-validation for robust evaluation\")\n",
    "\n",
    "print(f\"\\n   4. ğŸ“Š Model Evaluation:\")\n",
    "print(f\"      â€¢ Best model: {final_model_name}\")\n",
    "print(f\"      â€¢ Test RÂ² Score: {final_r2:.4f} ({final_r2*100:.1f}% variance explained)\")\n",
    "print(f\"      â€¢ Test MAE: {final_mae:.4f} points\")\n",
    "print(f\"      â€¢ Model explains real patterns in student performance!\")\n",
    "\n",
    "print(f\"\\n   5. ğŸ¯ Feature Analysis:\")\n",
    "if available_tree_models:\n",
    "    # Get top features from best tree model\n",
    "    if best_model_name in available_tree_models:\n",
    "        model = trained_models[best_model_name]\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            top_feature = importance_df.iloc[0]['feature']\n",
    "            print(f\"      â€¢ Most important feature: {top_feature}\")\n",
    "            print(f\"      â€¢ Feature importance analysis completed\")\n",
    "\n",
    "print(f\"\\n   6. ğŸ’¾ Model Persistence:\")\n",
    "print(f\"      â€¢ Saved trained model for deployment\")\n",
    "print(f\"      â€¢ Saved preprocessing scaler\")\n",
    "print(f\"      â€¢ Created prediction function\")\n",
    "print(f\"      â€¢ Saved model metadata and feature names\")\n",
    "\n",
    "print(f\"\\nğŸ“Š MODEL PERFORMANCE BREAKDOWN:\")\n",
    "print(f\"   â€¢ RÂ² Score of {final_r2:.4f} means:\")\n",
    "print(f\"     - Model explains {final_r2*100:.1f}% of exam score variance\")\n",
    "print(f\"     - Strong predictive power for student performance\")\n",
    "print(f\"   â€¢ MAE of {final_mae:.4f} means:\")\n",
    "print(f\"     - Average prediction error is Â±{final_mae:.1f} points\")\n",
    "print(f\"     - Very accurate for educational assessment\")\n",
    "\n",
    "print(f\"\\nğŸš€ NEXT STEPS:\")\n",
    "print(f\"   â€¢ Create model evaluation notebook (05_model_evaluation.ipynb)\")\n",
    "print(f\"   â€¢ Build web application for model deployment\")\n",
    "print(f\"   â€¢ Create interactive dashboard for predictions\")\n",
    "print(f\"   â€¢ Deploy model to help students improve performance\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ KEY INSIGHTS:\")\n",
    "print(f\"   â€¢ Machine learning can predict student performance accurately\")\n",
    "print(f\"   â€¢ Study habits and attendance are likely key factors\")\n",
    "print(f\"   â€¢ Model is ready for real-world application\")\n",
    "print(f\"   â€¢ Can help identify students who need extra support\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Congratulations! Your ML model is trained and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0bafcd",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully completed machine learning model training! You've learned:\n",
    "\n",
    "âœ… **Data Splitting** - Train/test splits and cross-validation  \n",
    "âœ… **Multiple Algorithms** - Linear, Tree-based, and Ensemble methods  \n",
    "âœ… **Model Evaluation** - RÂ², MAE, MSE, and residual analysis  \n",
    "âœ… **Feature Importance** - Understanding what drives predictions  \n",
    "âœ… **Hyperparameter Tuning** - Optimizing model performance  \n",
    "âœ… **Model Persistence** - Saving models for deployment  \n",
    "\n",
    "### ğŸš€ What's Next?\n",
    "\n",
    "In the next notebook (`04_model_evaluation.ipynb`), we'll dive deeper into:\n",
    "- Advanced model evaluation techniques\n",
    "- Error analysis and model diagnostics\n",
    "- Model interpretation and explainability\n",
    "- Preparing for deployment\n",
    "\n",
    "Then we'll create a web application in `05_model_deployment.ipynb`!\n",
    "\n",
    "Your model can now predict student exam scores with **{final_r2*100:.1f}% accuracy**! ğŸŒŸ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
